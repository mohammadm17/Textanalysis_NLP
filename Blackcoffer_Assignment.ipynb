{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7005a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Syllable Counting\n",
    "def syllable_counter(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    for i  in range(0,len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('es') or word.endswith('ed'):\n",
    "        count -= 1\n",
    "    if count< 0:\n",
    "        count == 0\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for counting Complex words\n",
    "def complex_word(word):\n",
    "    op = syllable_counter(word)\n",
    "    if op > 2:\n",
    "        return op\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting Personal pronouns\n",
    "def personal_counter(sent):\n",
    "    personal_list = ['I','we','We','My','my','Ours','ours','us']\n",
    "    count = 0\n",
    "    for i in personal_list:\n",
    "        if i in sent:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ab85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting input file into a Dataframe\n",
    "dataset = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Header variable\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Cafari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Stop Words\n",
    "\n",
    "directory = 'stop_words'\n",
    "stop_file = \"\"\n",
    "for i in os.listdir(directory):\n",
    "    f = os.path.join(directory,i)\n",
    "    stop_file += open(f).read()\n",
    "stop_words_list = nltk.word_tokenize(stop_file.lower())\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0470037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Postive & Negative Dictionary\n",
    "negative_dict = open(\"negative-words.txt\").read()\n",
    "token_negative_dict = nltk.word_tokenize(negative_dict)\n",
    "positive_dict = open(\"positive-words.txt\").read()\n",
    "token_positive_dict = nltk.word_tokenize(positive_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Lists to save output values\n",
    "Positive_Score = []\n",
    "Negative_Score = []\n",
    "Polarity_Score = []\n",
    "Subjectivity_Score= []\n",
    "avg_sentence_length = []\n",
    "avg_num_word_per_sentence = []\n",
    "percent_of_complex_words = []\n",
    "fog_index = []\n",
    "complex_word_count = []\n",
    "word_count = []\n",
    "syllable_per_word = []\n",
    "personal_pronouns = []\n",
    "avg_word_length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193623d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the loop for Scrapping data through the Urls given in the dataset \n",
    "for i in range(0,len(dataset)):\n",
    "    article_texts = \"\"\n",
    "    file_name = int(dataset['URL_ID'].iloc[i])\n",
    "    url = dataset['URL'].iloc[i]\n",
    "    try:\n",
    "        result = requests.get(url,headers = headers)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    soup = bs(result.content,'html.parser')\n",
    "\n",
    "    #Extracting article title\n",
    "    page_title= soup.title.text\n",
    "\n",
    "    # Saving Extracted article into Text files\n",
    "    try:\n",
    "        with open(str(file_name)+'.txt','a',encoding=\"utf-8\") as f:\n",
    "            f.write(page_title)\n",
    "            f.write('\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # Looping through all the Paragraphs in the article\n",
    "        for el in soup.find_all('p'):\n",
    "            article_texts += el.get_text()\n",
    "            f.write(el.get_text())\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "    tokenized_articles = nltk.word_tokenize(article_texts.lower())\n",
    "    cleaned_tokenized_article = tokenizer.tokenize(article_texts.lower())\n",
    "    \n",
    "    #Filtering the words present in the Stopwords\n",
    "    tokens_wot_sw = [word for word in tokenized_articles if not word in stop_words_list]\n",
    "    nltk_tokens_sw = [word for word in cleaned_tokenized_article if not word in nltk_stopwords]\n",
    "    \n",
    "    # Initializing Positive and Negative Index Counter\n",
    "    positive_index = 0\n",
    "    negative_index= 0\n",
    "    for j in tokens_wot_sw:\n",
    "        if j in token_positive_dict:\n",
    "            positive_index = positive_index+1\n",
    "\n",
    "        elif j in token_negative_dict:\n",
    "            negative_index = negative_index +1\n",
    "    \n",
    "    Positive_Score.append(positive_index)\n",
    "    Negative_Score.append(negative_index)\n",
    "    Polarity_Score.append((positive_index - negative_index)/ ((positive_index + negative_index) + 0.000001))\n",
    "    Subjectivity_Score.append((negative_index +  positive_index)/ (len(tokens_wot_sw) + 0.000001))\n",
    "    try:\n",
    "        avg_sentence_length.append(len(tokenized_articles)/len(sent_tokenize(article_texts)))\n",
    "    except ZeroDivisionError:\n",
    "        avg_sentence_length.append(0)\n",
    "    try:\n",
    "        syllable_per_word.append([syllable_counter(x) for x in tokenized_articles])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        complex_word_count.append([complex_word(x) for x in tokenized_articles])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        percent_of_complex_words.append([k/len(article_texts) for k in complex_word_count[-1]])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        fog_index.append([0.4*(avg_sentence_length[-1] + k) for k in percent_of_complex_words[-1]])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        word_count.append(len(cleaned_tokenized_article))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        personal_pronouns.append(personal_counter(article_texts))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        avg_num_word_per_sentence.append(len(tokenized_articles)/len(sent_tokenize(article_texts)))\n",
    "    except ZeroDivisionError:\n",
    "        avg_num_word_per_sentence.append(0)\n",
    "    try:\n",
    "        avg_word_length.append([len(i)/len(tokenized_articles) for i in tokenized_articles])\n",
    "    except ZeroDivisionError:\n",
    "        avg_word_length.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed35d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dataframe\n",
    "output_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe269a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping the values of the output columns with respective Outputs\n",
    "output_df = output_df.assign(**{'URL':dataset[\"URL\"],'POSITIVE SCORE':Positive_Score,'NEGATIVE SCORE':Negative_Score,'POLARITY SCORE':Polarity_Score,'SUBJECTIVITY SCORE':Subjectivity_Score,'AVG SENTENCE LENGTH':avg_sentence_length,'PERCENTAGE OF COMPLEX WORDS':percent_of_complex_words,'FOG INDEX':fog_index,'AVG NUMBER OF WORDS PER SENTENCE':avg_num_word_per_sentence,'COMPLEX WORD COUNT':complex_word_count,'WORD COUNT':word_count,'SYLLABLE PER WORD':syllable_per_word,'PERSONAL PRONOUNS':personal_pronouns,'AVG WORD LENGTH':avg_word_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Dataframe as a CSV file\n",
    "output_df.to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
